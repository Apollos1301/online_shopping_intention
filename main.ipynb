{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\shopping_intention\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] Die angegebene Prozedur wurde nicht gefunden'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 2.0000000e-01]\n",
      " [6.4000000e+01 0.0000000e+00]\n",
      " [0.0000000e+00 2.0000000e-01]\n",
      " ...\n",
      " [1.8425000e+02 8.3333333e-02]\n",
      " [3.4600000e+02 0.0000000e+00]\n",
      " [2.1250000e+01 0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/online_shoppers_intention.csv\")\n",
    "csv_data = pd.read_csv(\"./data/online_shoppers_intention.csv\")\n",
    "data.fillna(0, inplace=True)\n",
    "related_bounce = data.iloc[:,[5,6]]\n",
    "\n",
    "# print(related_bounce)\n",
    "related_bounce = related_bounce.values\n",
    "print(related_bounce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Returning_Visitor', 'New_Visitor', 'Other'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n",
    "data[\"VisitorType\"].unique()\n",
    "# dataFrame = data.drop(columns=['BounceRates'])\n",
    "# dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Administrative</th>\n",
       "      <th>Administrative_Duration</th>\n",
       "      <th>Informational</th>\n",
       "      <th>Informational_Duration</th>\n",
       "      <th>ProductRelated</th>\n",
       "      <th>ProductRelated_Duration</th>\n",
       "      <th>BounceRates</th>\n",
       "      <th>ExitRates</th>\n",
       "      <th>PageValues</th>\n",
       "      <th>SpecialDay</th>\n",
       "      <th>...</th>\n",
       "      <th>Month_Aug</th>\n",
       "      <th>Month_Dec</th>\n",
       "      <th>Month_Feb</th>\n",
       "      <th>Month_Jul</th>\n",
       "      <th>Month_June</th>\n",
       "      <th>Month_Mar</th>\n",
       "      <th>Month_May</th>\n",
       "      <th>Month_Nov</th>\n",
       "      <th>Month_Oct</th>\n",
       "      <th>Month_Sep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>627.500000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12325</th>\n",
       "      <td>3</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53</td>\n",
       "      <td>1783.791667</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.029031</td>\n",
       "      <td>12.241717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12326</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>465.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12327</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>184.250000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12328</th>\n",
       "      <td>4</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12329</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12330 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Administrative  Administrative_Duration  Informational  \\\n",
       "0                   0                      0.0              0   \n",
       "1                   0                      0.0              0   \n",
       "2                   0                      0.0              0   \n",
       "3                   0                      0.0              0   \n",
       "4                   0                      0.0              0   \n",
       "...               ...                      ...            ...   \n",
       "12325               3                    145.0              0   \n",
       "12326               0                      0.0              0   \n",
       "12327               0                      0.0              0   \n",
       "12328               4                     75.0              0   \n",
       "12329               0                      0.0              0   \n",
       "\n",
       "       Informational_Duration  ProductRelated  ProductRelated_Duration  \\\n",
       "0                         0.0               1                 0.000000   \n",
       "1                         0.0               2                64.000000   \n",
       "2                         0.0               1                 0.000000   \n",
       "3                         0.0               2                 2.666667   \n",
       "4                         0.0              10               627.500000   \n",
       "...                       ...             ...                      ...   \n",
       "12325                     0.0              53              1783.791667   \n",
       "12326                     0.0               5               465.750000   \n",
       "12327                     0.0               6               184.250000   \n",
       "12328                     0.0              15               346.000000   \n",
       "12329                     0.0               3                21.250000   \n",
       "\n",
       "       BounceRates  ExitRates  PageValues  SpecialDay  ...  Month_Aug  \\\n",
       "0         0.200000   0.200000    0.000000         0.0  ...          0   \n",
       "1         0.000000   0.100000    0.000000         0.0  ...          0   \n",
       "2         0.200000   0.200000    0.000000         0.0  ...          0   \n",
       "3         0.050000   0.140000    0.000000         0.0  ...          0   \n",
       "4         0.020000   0.050000    0.000000         0.0  ...          0   \n",
       "...            ...        ...         ...         ...  ...        ...   \n",
       "12325     0.007143   0.029031   12.241717         0.0  ...          0   \n",
       "12326     0.000000   0.021333    0.000000         0.0  ...          0   \n",
       "12327     0.083333   0.086667    0.000000         0.0  ...          0   \n",
       "12328     0.000000   0.021053    0.000000         0.0  ...          0   \n",
       "12329     0.000000   0.066667    0.000000         0.0  ...          0   \n",
       "\n",
       "       Month_Dec  Month_Feb  Month_Jul  Month_June  Month_Mar  Month_May  \\\n",
       "0              0          1          0           0          0          0   \n",
       "1              0          1          0           0          0          0   \n",
       "2              0          1          0           0          0          0   \n",
       "3              0          1          0           0          0          0   \n",
       "4              0          1          0           0          0          0   \n",
       "...          ...        ...        ...         ...        ...        ...   \n",
       "12325          1          0          0           0          0          0   \n",
       "12326          0          0          0           0          0          0   \n",
       "12327          0          0          0           0          0          0   \n",
       "12328          0          0          0           0          0          0   \n",
       "12329          0          0          0           0          0          0   \n",
       "\n",
       "       Month_Nov  Month_Oct  Month_Sep  \n",
       "0              0          0          0  \n",
       "1              0          0          0  \n",
       "2              0          0          0  \n",
       "3              0          0          0  \n",
       "4              0          0          0  \n",
       "...          ...        ...        ...  \n",
       "12325          0          0          0  \n",
       "12326          1          0          0  \n",
       "12327          1          0          0  \n",
       "12328          1          0          0  \n",
       "12329          1          0          0  \n",
       "\n",
       "[12330 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customers = data[\"VisitorType\"]\n",
    "df_customers_dict = {\"New_Visitor\":0,\n",
    "                     \"Returning_Visitor\":1,\n",
    "                     \"Other\":0}\n",
    "df_customers = df_customers.map(lambda x: df_customers_dict[x])\n",
    "df = data.drop(columns=['OperatingSystems', \"Browser\"])\n",
    "# hot_encoded_dict = \n",
    "df = pd.get_dummies(df, columns=[\"Month\"])\n",
    "df[\"VisitorType\"] = df[\"VisitorType\"].map(lambda x: df_customers_dict[x])\n",
    "df = df.map(lambda x: int(x) if isinstance(x, bool) else x)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Elbow to determine optimal cluster size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 12):\n",
    "    k_means = KMeans(n_clusters= i,\n",
    "                     init= 'k-means++',\n",
    "                     max_iter= 300,\n",
    "                     n_init= 10,\n",
    "                     random_state= 0,\n",
    "                     tol= 0.001)\n",
    "    k_means.fit(related_bounce)\n",
    "    lables = k_means.labels_\n",
    "    wcss.append(k_means.inertia_)\n",
    "\n",
    "plt.figure(figsize=(13,7))\n",
    "plt.plot(wcss, marker='o', linestyle='dashed')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Sum of Distances')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = KMeans(n_clusters= 2,init= 'k-means++',max_iter= 300,n_init= 10,random_state= 0,tol= 0.001)\n",
    "y_clusters = k_means.fit_predict(related_bounce)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.title('Product Related Duration vs Bounce Rates')\n",
    "plt.grid()\n",
    "plt.scatter(related_bounce[y_clusters == 0, 0], related_bounce[y_clusters == 0, 1], c='yellow', label=\"Uninteresting Customers\")\n",
    "plt.scatter(related_bounce[y_clusters == 1, 0], related_bounce[y_clusters == 1, 1], c='green', label=\"Target Customers\")\n",
    "plt.scatter(k_means.cluster_centers_[:,0], k_means.cluster_centers_[:,1], c='red',label=\"Centroids\", marker='+')\n",
    "plt.xlabel('Product Related Duration')\n",
    "plt.ylabel('Bounce Rates')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {0:\"red\",1:\"green\",2:\"blue\",3:\"purple\",4:\"black\",5:\"pink\",6:\"brown\",7:\"olive\",8:\"orange\"}\n",
    "\n",
    "k_means = KMeans(n_clusters= 2,init= 'k-means++',max_iter= 300,n_init= 10,random_state= 0,tol= 0.001)\n",
    "y_clusters = k_means.fit_predict(related_bounce)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 2 Rows, 1 Column\n",
    "\n",
    "# Plotting on the first subplot\n",
    "for i in range(2):\n",
    "    axs[0].scatter(related_bounce[y_clusters == i ,0],related_bounce[y_clusters == i ,1],  c=color_dict[i], label='Customer type: '+str(i))  # Example plot\n",
    "axs[0].set_title('Product Related Duration vs Bounce Rates')  # Title for the first plot\n",
    "axs[0].set_xlabel('Product Related Duration')\n",
    "axs[0].set_ylabel('Bounce Rates')\n",
    "axs[0].legend()\n",
    "\n",
    "\n",
    "# for i in range(3):\n",
    "#     axs[1].scatter(related_bounce[y_clusters == i ,0],related_bounce[y_clusters == i ,1],  c=df_customers.values, label='Customer type: '+str(i))  # Example plot\n",
    "axs[1].scatter(related_bounce[:,0],related_bounce[:,1], c=df_customers.values)\n",
    "axs[1].set_title('Product Related Duration vs Bounce Rates')  # Title for the first plot\n",
    "axs[1].set_xlabel('Product Related Duration')\n",
    "axs[1].grid()\n",
    "axs[1].set_ylabel('Bounce Rates')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "color_dict = {0:\"red\",1:\"green\",2:\"blue\",3:\"purple\",4:\"black\",5:\"pink\",6:\"brown\",7:\"olive\",8:\"orange\"}\n",
    "\n",
    "k_means = KMeans(n_clusters= len(df[\"Region\"].unique()),init= 'k-means++',max_iter= 300,n_init= 10,random_state= 0,tol= 0.001)\n",
    "y_clusters = k_means.fit_predict(related_bounce)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axs[0].set_title('Product Related Duration vs Bounce Rates')\n",
    "axs[0].grid()\n",
    "for i in range(len(df[\"Region\"].unique())):\n",
    "    axs[0].scatter(related_bounce[y_clusters == i ,0],related_bounce[y_clusters == i ,1],  c=color_dict[i], label='Region: '+str(i))\n",
    "axs[0].set_xlabel('Product Related Duration')\n",
    "axs[0].set_ylabel('Bounce Rates')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_title('Product Related Duration vs Bounce Rates')\n",
    "axs[1].grid()\n",
    "# for i in range(len(df[\"Region\"].unique())):\n",
    "#     axs[1].scatter(related_bounce[y_clusters == i ,0],related_bounce[y_clusters == i ,1],  c=color_dict[i], label='Region: '+str(i))\n",
    "axs[1].scatter(related_bounce[:,0],related_bounce[:,1],  c=df[\"Region\"].values)\n",
    "axs[1].set_xlabel('Product Related Duration')\n",
    "axs[1].set_ylabel('Bounce Rates')\n",
    "\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data = df\n",
    "# pca_data = (pca_data - pca_data.mean()) / pca_data.std()\n",
    "# print(pca_data)\n",
    "pca_data=(pca_data-pca_data.min())/(pca_data.max()-pca_data.min())\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "x = pca.fit_transform(pca_data)\n",
    "pca_data[\"VisitorType\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "axs[0].grid()\n",
    "axs[0].scatter(x[pca_data[\"VisitorType\"].values == 0, 0], x[pca_data[\"VisitorType\"].values == 0, 1], c=\"blue\", label=\"new/other customers\")\n",
    "axs[0].scatter(x[pca_data[\"VisitorType\"].values == 1, 0], x[pca_data[\"VisitorType\"].values == 1, 1], c=\"green\", label=\"returning customers\")\n",
    "axs[0].legend()\n",
    "\n",
    "k_means = KMeans(n_clusters= 2,init= 'k-means++',max_iter= 300,n_init= 10,random_state= 0,tol= 0.001)\n",
    "y_clusters = k_means.fit_predict(x)\n",
    "\n",
    "axs[1].grid()\n",
    "axs[1].scatter(x[:,0], x[:,1], c=y_clusters)\n",
    "axs[1].scatter(k_means.cluster_centers_[:,0],k_means.cluster_centers_[:,1], c=\"blue\", marker=\"+\")\n",
    "\n",
    "axs[2].grid()\n",
    "axs[2].scatter(x[pca_data[\"Revenue\"].values == 1,0], x[pca_data[\"Revenue\"].values == 1,1])\n",
    "\n",
    "plt.tight_layout()  \n",
    "plt.show()\n",
    "print(f'{round((len(pca_data[pca_data[\"Revenue\"] == 0].Revenue.values) / len(pca_data[\"Revenue\"]))*100, 2)} %')\n",
    "print(f'{round((len(pca_data[pca_data[\"Revenue\"] == 1].Revenue.values)/ len(pca_data[\"Revenue\"]))*100, 2)} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "x = pca.fit_transform(pca_data)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    " \n",
    "axis = fig.add_subplot(111, projection='3d')\n",
    "axis.scatter(x[:,0],x[:,1],x[:,2], c=pca_data[\"Revenue\"].values,cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        torch.set_default_dtype(torch.float64)\n",
    "        self.linear1 = nn.Linear(input_size, 16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.linear2 = nn.Linear(16, 8)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(8,1)\n",
    "        # self.relu3 = nn.ReLU()\n",
    "        # self.linear4 = nn.Linear(6,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_ = self.linear1(x)\n",
    "        x_ = self.relu1(x_)\n",
    "        x = self.dropout1(x)\n",
    "        x_ = self.linear2(x_)\n",
    "        x_ = self.relu2(x_)\n",
    "        x_ = self.linear3(x_)\n",
    "        # x_ = self.relu3(x_)\n",
    "        # x_ = self.linear4(x_)\n",
    "        x_ = self.sigmoid(x_)\n",
    "        return x_\n",
    "    \n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, dataFrame, class_column = \"Revenue\"):\n",
    "        df = dataFrame.drop(columns=[class_column])\n",
    "        # self.labels = torch.tensor(dataFrame[\"Revenue\"].values)\n",
    "        # self.labels = self.labels.double()\n",
    "        # self.data = torch.tensor(df.values, dtype=torch.double)\n",
    "        self.labels = dataFrame[class_column].values\n",
    "        self.data = df.values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data_tensor = self.data[idx]\n",
    "        labels_tensor = self.labels[idx]\n",
    "        return data_tensor, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int(len(df) * 0.8)\n",
    "df = (df-df.min())/(df.max()-df.min())\n",
    "# df = (df - df.mean()) / df.std()\n",
    "\n",
    "train_data = df.iloc[:train_len]\n",
    "test_data = df.iloc[train_len:]\n",
    "train_set = CustomDataSet(train_data)\n",
    "test_set = CustomDataSet(test_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=5, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=5, shuffle=True)\n",
    "# data, label = dataset[0]\n",
    "# print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: 24\n",
      "1973\n",
      "494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\shopping_intention\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "input_size = len(df.drop(columns=[\"VisitorType\"]).columns)\n",
    "print(f'input size: {input_size}')\n",
    "model = MLP(input_size).to(device=device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,min_lr=1e-6, factor=0.1, verbose=True)\n",
    "print(len(train_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for data, label in test_dataloader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            label = label.unsqueeze(1)  # Ensure labels have the correct shape\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Converting probabilities to binary predictions (0 or 1)\n",
    "            predicted_classes = (output > 0.5).float()\n",
    "            \n",
    "            predictions.extend(predicted_classes.view(-1).cpu().numpy())\n",
    "            actuals.extend(label.view(-1).cpu().numpy())\n",
    "            # Counting correct predictions\n",
    "            correct_predictions += (predicted_classes == label).sum().item()\n",
    "            total_predictions += label.size(0)\n",
    "        \n",
    "        # Calculating the accuracy\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        print(f'Accuracy on test set: {accuracy * 100:.2f}%')\n",
    "        \n",
    "        roc_auc = roc_auc_score(actuals, predictions)\n",
    "        print(f'ROC AUC Score: {roc_auc:.2f}')\n",
    "        return roc_auc, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Epoch 1/64, Loss: 0.3693 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 81.47%\n",
      "ROC AUC Score: 0.54\n",
      "***************************************\n",
      "*saved at acc: 0.8147, ROC AUC: 0.5443*\n",
      "***************************************\n",
      "----------------------------------------------------------------\n",
      "Epoch 2/64, Loss: 0.3261 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 83.74%\n",
      "ROC AUC Score: 0.72\n",
      "***************************************\n",
      "*saved at acc: 0.8374, ROC AUC: 0.7231*\n",
      "***************************************\n",
      "----------------------------------------------------------------\n",
      "Epoch 3/64, Loss: 0.2997 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 83.01%\n",
      "ROC AUC Score: 0.60\n",
      "----------------------------------------------------------------\n",
      "Epoch 4/64, Loss: 0.2839 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 84.59%\n",
      "ROC AUC Score: 0.65\n",
      "----------------------------------------------------------------\n",
      "Epoch 5/64, Loss: 0.2764 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 64.52%\n",
      "ROC AUC Score: 0.73\n",
      "***************************************\n",
      "*saved at acc: 0.6452, ROC AUC: 0.7334*\n",
      "***************************************\n",
      "----------------------------------------------------------------\n",
      "Epoch 6/64, Loss: 0.2703 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 83.82%\n",
      "ROC AUC Score: 0.63\n",
      "----------------------------------------------------------------\n",
      "Epoch 7/64, Loss: 0.2669 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.16%\n",
      "ROC AUC Score: 0.73\n",
      "----------------------------------------------------------------\n",
      "Epoch 8/64, Loss: 0.2627 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 84.91%\n",
      "ROC AUC Score: 0.69\n",
      "----------------------------------------------------------------\n",
      "Epoch 9/64, Loss: 0.2588 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 83.66%\n",
      "ROC AUC Score: 0.62\n",
      "----------------------------------------------------------------\n",
      "Epoch 10/64, Loss: 0.254 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 84.14%\n",
      "ROC AUC Score: 0.64\n",
      "----------------------------------------------------------------\n",
      "Epoch 11/64, Loss: 0.2546 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.12%\n",
      "ROC AUC Score: 0.75\n",
      "***************************************\n",
      "*saved at acc: 0.8512, ROC AUC: 0.7514*\n",
      "***************************************\n",
      "----------------------------------------------------------------\n",
      "Epoch 12/64, Loss: 0.2512 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.16%\n",
      "ROC AUC Score: 0.68\n",
      "----------------------------------------------------------------\n",
      "Epoch 13/64, Loss: 0.2506 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.64%\n",
      "ROC AUC Score: 0.70\n",
      "----------------------------------------------------------------\n",
      "Epoch 14/64, Loss: 0.2473 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 84.39%\n",
      "ROC AUC Score: 0.64\n",
      "----------------------------------------------------------------\n",
      "Epoch 15/64, Loss: 0.2441 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.93%\n",
      "ROC AUC Score: 0.70\n",
      "----------------------------------------------------------------\n",
      "Epoch 16/64, Loss: 0.2428 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 84.67%\n",
      "ROC AUC Score: 0.66\n",
      "----------------------------------------------------------------\n",
      "Epoch 17/64, Loss: 0.2419 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.00%\n",
      "ROC AUC Score: 0.67\n",
      "----------------------------------------------------------------\n",
      "Epoch 18/64, Loss: 0.24 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 84.55%\n",
      "ROC AUC Score: 0.66\n",
      "----------------------------------------------------------------\n",
      "Epoch 19/64, Loss: 0.237 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 84.71%\n",
      "ROC AUC Score: 0.65\n",
      "----------------------------------------------------------------\n",
      "Epoch 20/64, Loss: 0.2382 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 84.83%\n",
      "ROC AUC Score: 0.67\n",
      "----------------------------------------------------------------\n",
      "Epoch 21/64, Loss: 0.2352 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 86.13%\n",
      "ROC AUC Score: 0.73\n",
      "----------------------------------------------------------------\n",
      "Epoch 22/64, Loss: 0.2362 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 86.54%\n",
      "ROC AUC Score: 0.74\n",
      "----------------------------------------------------------------\n",
      "Epoch 23/64, Loss: 0.2341 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 86.01%\n",
      "ROC AUC Score: 0.72\n",
      "----------------------------------------------------------------\n",
      "Epoch 24/64, Loss: 0.2342 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 83.90%\n",
      "ROC AUC Score: 0.72\n",
      "----------------------------------------------------------------\n",
      "Epoch 25/64, Loss: 0.2339 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.48%\n",
      "ROC AUC Score: 0.77\n",
      "***************************************\n",
      "*saved at acc: 0.8548, ROC AUC: 0.7651*\n",
      "***************************************\n",
      "----------------------------------------------------------------\n",
      "Epoch 26/64, Loss: 0.2325 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.04%\n",
      "ROC AUC Score: 0.67\n",
      "----------------------------------------------------------------\n",
      "Epoch 27/64, Loss: 0.2327 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 86.25%\n",
      "ROC AUC Score: 0.73\n",
      "----------------------------------------------------------------\n",
      "Epoch 28/64, Loss: 0.2291 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 83.78%\n",
      "ROC AUC Score: 0.77\n",
      "***************************************\n",
      "*saved at acc: 0.8378, ROC AUC: 0.7689*\n",
      "***************************************\n",
      "----------------------------------------------------------------\n",
      "Epoch 29/64, Loss: 0.2296 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.24%\n",
      "ROC AUC Score: 0.67\n",
      "----------------------------------------------------------------\n",
      "Epoch 30/64, Loss: 0.2279 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.85%\n",
      "ROC AUC Score: 0.76\n",
      "----------------------------------------------------------------\n",
      "Epoch 31/64, Loss: 0.2296 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.93%\n",
      "ROC AUC Score: 0.73\n",
      "----------------------------------------------------------------\n",
      "Epoch 32/64, Loss: 0.2284 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.81%\n",
      "ROC AUC Score: 0.69\n",
      "----------------------------------------------------------------\n",
      "Epoch 33/64, Loss: 0.226 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.77%\n",
      "ROC AUC Score: 0.69\n",
      "----------------------------------------------------------------\n",
      "Epoch 34/64, Loss: 0.2253 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 84.51%\n",
      "ROC AUC Score: 0.74\n",
      "----------------------------------------------------------------\n",
      "Epoch 35/64, Loss: 0.2229 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.24%\n",
      "ROC AUC Score: 0.67\n",
      "----------------------------------------------------------------\n",
      "Epoch 36/64, Loss: 0.2248 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 86.05%\n",
      "ROC AUC Score: 0.75\n",
      "----------------------------------------------------------------\n",
      "Epoch 37/64, Loss: 0.2238 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.20%\n",
      "ROC AUC Score: 0.68\n",
      "----------------------------------------------------------------\n",
      "Epoch 38/64, Loss: 0.2241 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.89%\n",
      "ROC AUC Score: 0.71\n",
      "----------------------------------------------------------------\n",
      "Epoch 39/64, Loss: 0.2228 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.48%\n",
      "ROC AUC Score: 0.68\n",
      "----------------------------------------------------------------\n",
      "Epoch 40/64, Loss: 0.2241 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.93%\n",
      "ROC AUC Score: 0.70\n",
      "----------------------------------------------------------------\n",
      "Epoch 41/64, Loss: 0.2232 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.52%\n",
      "ROC AUC Score: 0.70\n",
      "----------------------------------------------------------------\n",
      "Epoch 42/64, Loss: 0.2242 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.56%\n",
      "ROC AUC Score: 0.68\n",
      "----------------------------------------------------------------\n",
      "Epoch 43/64, Loss: 0.2235 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.48%\n",
      "ROC AUC Score: 0.69\n",
      "----------------------------------------------------------------\n",
      "Epoch 44/64, Loss: 0.2203 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 86.01%\n",
      "ROC AUC Score: 0.71\n",
      "----------------------------------------------------------------\n",
      "Epoch 45/64, Loss: 0.2195 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 86.13%\n",
      "ROC AUC Score: 0.73\n",
      "----------------------------------------------------------------\n",
      "Epoch 46/64, Loss: 0.2213 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 84.67%\n",
      "ROC AUC Score: 0.64\n",
      "----------------------------------------------------------------\n",
      "Epoch 47/64, Loss: 0.2198 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 86.25%\n",
      "ROC AUC Score: 0.74\n",
      "----------------------------------------------------------------\n",
      "Epoch 48/64, Loss: 0.2202 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.73%\n",
      "ROC AUC Score: 0.69\n",
      "----------------------------------------------------------------\n",
      "Epoch 49/64, Loss: 0.219 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.52%\n",
      "ROC AUC Score: 0.68\n",
      "----------------------------------------------------------------\n",
      "Epoch 50/64, Loss: 0.2193 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 86.42%\n",
      "ROC AUC Score: 0.73\n",
      "----------------------------------------------------------------\n",
      "Epoch 51/64, Loss: 0.2202 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.36%\n",
      "ROC AUC Score: 0.67\n",
      "----------------------------------------------------------------\n",
      "Epoch 52/64, Loss: 0.2194 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.69%\n",
      "ROC AUC Score: 0.68\n",
      "----------------------------------------------------------------\n",
      "Epoch 53/64, Loss: 0.2188 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 86.21%\n",
      "ROC AUC Score: 0.70\n",
      "----------------------------------------------------------------\n",
      "Epoch 54/64, Loss: 0.2169 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 86.50%\n",
      "ROC AUC Score: 0.74\n",
      "----------------------------------------------------------------\n",
      "Epoch 55/64, Loss: 0.2175 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 86.37%\n",
      "ROC AUC Score: 0.71\n",
      "----------------------------------------------------------------\n",
      "Epoch 56/64, Loss: 0.2177 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.32%\n",
      "ROC AUC Score: 0.67\n",
      "----------------------------------------------------------------\n",
      "Epoch 57/64, Loss: 0.2168 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 86.09%\n",
      "ROC AUC Score: 0.73\n",
      "----------------------------------------------------------------\n",
      "Epoch 58/64, Loss: 0.2168 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.48%\n",
      "ROC AUC Score: 0.74\n",
      "----------------------------------------------------------------\n",
      "Epoch 59/64, Loss: 0.2158 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 86.01%\n",
      "ROC AUC Score: 0.73\n",
      "----------------------------------------------------------------\n",
      "Epoch 60/64, Loss: 0.2168 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 86.01%\n",
      "ROC AUC Score: 0.70\n",
      "----------------------------------------------------------------\n",
      "Epoch 61/64, Loss: 0.2169 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.64%\n",
      "ROC AUC Score: 0.68\n",
      "----------------------------------------------------------------\n",
      "Epoch 62/64, Loss: 0.2149 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.73%\n",
      "ROC AUC Score: 0.69\n",
      "----------------------------------------------------------------\n",
      "Epoch 63/64, Loss: 0.2158 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 85.93%\n",
      "ROC AUC Score: 0.72\n",
      "----------------------------------------------------------------\n",
      "Epoch 64/64, Loss: 0.2138 | SGD lr 0.1 -> 0.1\n",
      "Accuracy on test set: 86.01%\n",
      "ROC AUC Score: 0.71\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 64\n",
    "PATH = \"./model/\"\n",
    "loss_values = []\n",
    "total_loss = 0\n",
    "best_roc_auc = 0\n",
    "best_model = copy.deepcopy(model.state_dict())\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    model.train()\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    for data, label in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        label = label.unsqueeze(1)\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        pred_classes = (output > 0.5).float()\n",
    "        actuals.extend(label.view(-1).cpu().numpy())\n",
    "        predictions.extend(pred_classes.view(-1).cpu().numpy())\n",
    "    \n",
    "    \n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    scheduler.step(average_loss)\n",
    "    after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    \n",
    "    loss_values.append(average_loss)\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {round(average_loss, 4)} | SGD lr {round(before_lr, 4)} -> {round(after_lr, 4)}')\n",
    "    roc_auc, accuracy = test_model(model)\n",
    "    if best_roc_auc <= roc_auc:\n",
    "        best_roc_auc = roc_auc\n",
    "        if os.path.exists(PATH+\"bestModel.pt\"):\n",
    "          os.remove(PATH+\"bestModel.pt\")\n",
    "        best_model = copy.deepcopy(model.state_dict())\n",
    "        print(f'***************************************')\n",
    "        print(f'*saved at acc: {round(accuracy,4)}, ROC AUC: {round(roc_auc, 4)}*')\n",
    "        print(f'***************************************')\n",
    "    # scheduler.step(roc_auc)\n",
    "    # roc_auc = roc_auc_score(actuals, predictions)\n",
    "    # print(f'ROC AUC Score: {roc_auc:.2f}')\n",
    "\n",
    "model.load_state_dict(best_model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = model.linear1.weight.cpu().detach().numpy()\n",
    "\n",
    "# # Summing the absolute weights for each input feature\n",
    "# feature_importance = np.sum(weights, axis=0)\n",
    "\n",
    "# # Sorting features by importance\n",
    "# sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "# print(\"Feature importance ranking (from most to least important):\")\n",
    "# print(sorted_idx)\n",
    "\n",
    "# print(train_data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 83.78%\n",
      "ROC AUC Score: 0.77\n"
     ]
    }
   ],
   "source": [
    "plt.plot(range(1, NUM_EPOCHS + 1), loss_values, marker='o')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.show()\n",
    "roc_auc, accuracy = test_model(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 1.0\n",
      "Predicted class: 0.0\n",
      "Actual class: 0.0\n",
      "Predicted class: 0.0\n",
      "31\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_visitor_set = csv_data\n",
    "new_visitor_set = new_visitor_set.drop(columns=['OperatingSystems', \"Browser\"])\n",
    "# hot_encoded_dict = \n",
    "new_visitor_set = pd.get_dummies(new_visitor_set, columns=[\"Month\"])\n",
    "new_visitor_set = new_visitor_set.iloc[train_len:]\n",
    "new_visitor_set = new_visitor_set[new_visitor_set[\"VisitorType\"] == \"New_Visitor\"]\n",
    "new_visitor_set[\"VisitorType\"] = new_visitor_set[\"VisitorType\"].map(lambda x: df_customers_dict[x])\n",
    "\n",
    "new_visitor_set = new_visitor_set.map(lambda x: int(x) if isinstance(x, bool) else x)\n",
    "\n",
    "# new_visitor_set[new_visitor_set[\"Revenue\"] == 1]\n",
    "new_visitor_set = (new_visitor_set-new_visitor_set.min())/(new_visitor_set.max()-new_visitor_set.min())\n",
    "new_visitor_set[\"VisitorType\"] = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# new_visitor_set = (new_visitor_set - new_visitor_set.mean()) / new_visitor_set.std()\n",
    "# IDX = 1\n",
    "# print(new_visitor_set[new_visitor_set[\"Revenue\"] == 1])\n",
    "all_true = 0\n",
    "true_pos = 0\n",
    "# print(new_visitor_set[\"Revenue\"].values[:100])\n",
    "for IDX in range(100):\n",
    "    test_sample_y = new_visitor_set.iloc[IDX][\"Revenue\"]\n",
    "    test_sample = new_visitor_set.iloc[IDX]\n",
    "    test_sample = test_sample.drop('Revenue').values\n",
    "\n",
    "    test_sample = torch.tensor(test_sample).to(device)\n",
    "    test_sample_y = torch.tensor(test_sample_y).float().to(device)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(test_sample)\n",
    "\n",
    "        # Converting probabilities to binary predictions (0 or 1)\n",
    "        predicted_classes = (output > 0.5).float()\n",
    "        predicted_classes = predicted_classes.unsqueeze(-1)\n",
    "        print(f'Actual class: {test_sample_y.item()}')\n",
    "        print(f'Predicted class: {predicted_classes.item()}')\n",
    "        \n",
    "        if test_sample_y.item() == 1:\n",
    "            all_true += 1\n",
    "            if predicted_classes.item() == 1:\n",
    "                true_pos += 1\n",
    "                \n",
    "print(all_true)\n",
    "print(true_pos)\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shopping_intention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
